

---

````markdown
# ğŸ§  Universal FastAPI Backend for ML Inference

A **plug-and-play backend template** using FastAPI that supports **multi-modal model inference** â€” with standardized input/output handling for seamless frontend integration.

Built to work effortlessly with any ML model (e.g., summarization, text-to-speech, transcription), the only thing you need to change is the **core model logic**, while the rest of the backend remains constant.

---

## ğŸš€ Features

- âœ… Accepts multiple input types (text, audio)
- âœ… Supports output as text (JSON) or audio (MPEG stream)
- âœ… Minimal changes required for switching tasks
- âœ… Fully compatible with a React + TypeScript frontend
- âœ… Easily extensible and production-ready

---

## ğŸ“¦ Installation

```bash
git clone <your-repo-url>
cd <project-folder>
npm install
npm run dev
````

---

## ğŸ Running the Server

```bash
uvicorn main:app --reload
```

Server runs locally on:
ğŸ“ `http://127.0.0.1:8000`

---

## ğŸ§° API Endpoint

### `POST /infer`

Send a multipart `FormData` request with:

| Field         | Type     | Required | Description                   |
| ------------- | -------- | -------- | ----------------------------- |
| `input_name`  | `string` | optional | One or more text input fields |
| `file_name`   | `file`   | optional | One or more audio file inputs |
| `output_type` | `string` | yes      | Either `"text"` or `"audio"`  |

---

## ğŸ”„ Standard Template (Do Not Modify)

In `main.py`, the structure is fixed:

```python
@app.post("/infer")
async def infer(request: Request, output_type: str = Form(...)):
    form = await request.form()

    input_data = {}
    audio_inputs = {}

    for key, value in form.multi_items():
        if key == "output_type":
            continue
        if isinstance(value, UploadFile):
            audio_inputs[key] = value
        else:
            input_data[key] = value

    # <<< CUSTOM LOGIC SECTION >>>
```

---

## âœ¨ Examples

### ğŸ“ Summarization

```python
from transformers import pipeline
summarizer = pipeline("summarization")
summary = summarizer(input_data["text"], max_length=60)[0]["summary_text"]
return JSONResponse(content={"text": summary})
```

---

### ğŸ”Š Text to Speech (TTS)

```python
from gtts import gTTS
tts = gTTS(" ".join(input_data.values()))
audio_fp = io.BytesIO()
tts.write_to_fp(audio_fp)
audio_fp.seek(0)
return StreamingResponse(audio_fp, media_type="audio/mpeg")
```

---

### ğŸ¤ Whisper Transcription

```python
import whisper
model = whisper.load_model("base")
file = next(iter(audio_inputs.values()))
with open("temp.wav", "wb") as f:
    f.write(await file.read())
result = model.transcribe("temp.wav")
return JSONResponse(content={"text": result["text"]})
```

---

## ğŸŒ Frontend Integration

Use this backend with your React/TypeScript frontend that sends a `FormData` request like:

```ts
formData.append("text_input", "Hello World!");
formData.append("output_type", "audio");
```

> âš ï¸ Backend must be CORS-enabled when deployed for production use. If not local

---

## ğŸ“ Example Folder Structure

```
project/
â”‚
â”œâ”€â”€ main.py                  # FastAPI backend
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md                # Project documentation
```

---

## ğŸ“Œ Future Improvements

* Token-based authentication
* S3 integration for output storage
* Dockerize and deploy on Fly.io or Render
* Real-time model inference stats/logs

---

## ğŸ‘¨â€ğŸ’» Author

Built by [Anirudh @ IIIT Hyderabad](anirudh.bocha@students,iiit.ac.in)

---

## ğŸ“ License

MIT License. Use freely and modify as needed.

---
